# coding: utf-8
#
# user-graph need to be generated by the following script
# tools/generate-u-u-matrix.py
import os
import numpy as np
import scipy.sparse as sp
import torch
import torch.nn as nn
import torch.nn.functional as F

from common.abstract_recommender import GeneralRecommender
from common.loss import BPRLoss, EmbLoss
from common.init import xavier_uniform_initialization
from utils.svd_completion import apply_svd_completion_to_dataset
from utils.fusion_similarity import build_fusion_knn_graph
from utils.adaptive_bpr import AdaptiveBPRLoss

# Removed ProjectHead class - not mentioned in the SEA paper

class CLUB(nn.Module):
    """
    Contrastive Log-ratio Upper Bound (CLUB) as per SEA paper Equation 15
    """
    def __init__(self):
        super(CLUB, self).__init__()

    def forward(self, generic, unique):
        # Paper's Equation 15: CLUB upper bound for mutual information minimization
        pos = F.mse_loss(generic, unique, reduction='mean')
        # Shuffle for negative samples
        shuffled = unique[torch.randperm(unique.size(0))]
        neg = F.mse_loss(generic, shuffled, reduction='mean')
        
        # Add stability: clamp the result to prevent explosion
        result = pos - neg
        return torch.clamp(result, min=-10.0, max=10.0)  # Prevent extreme values
    
class SEA(GeneralRecommender):
    def __init__(self, config, dataset, args):
        super(SEA, self).__init__(config, dataset)

        num_user = self.n_users
        num_item = self.n_items
        batch_size = config['train_batch_size']  # not used
        dim_x = config['embedding_size']
        self.feat_embed_dim = config['feat_embed_dim']
        self.n_layers = config['n_mm_layers']
        self.knn_k = config['knn_k']
        self.mm_image_weight = config['mm_image_weight']

        # SEA paper parameters - simplified
        self.alpha_contrast = args.alpha_contrast
        self.temp = args.temp
        self.beta = args.beta
        
        # Core model parameters
        self.batch_size = batch_size
        self.num_user = num_user
        self.num_item = num_item
        self.aggr_mode = 'add'
        self.dataset = dataset
        self.dropout = config['dropout']
        self.reg_weight = config['reg_weight']

        # Paper Section 4.2.1: Learned splitting layers for modal-unique and modal-generic
        # Instead of simple tensor slicing, use learned transformations
        self.v_generic_split = nn.Linear(dim_x, dim_x // 2)
        self.v_unique_split = nn.Linear(dim_x, dim_x // 2)
        self.t_generic_split = nn.Linear(dim_x, dim_x // 2)
        self.t_unique_split = nn.Linear(dim_x, dim_x // 2)

        # Paper Equation 15: CLUB instances for mutual information minimization
        self.club_v = CLUB()  # For visual modality distancing
        self.club_t = CLUB()  # For text modality distancing

        # Initialize multimodal adjacency matrix for item-item graph (homogeneous)
        dataset_path = os.path.abspath(config['data_path'] + config['dataset'])
        mm_adj_file = os.path.join(dataset_path, 'mm_adj_{}.pt'.format(self.knn_k))

        if self.v_feat is not None:
            self.image_embedding = nn.Embedding.from_pretrained(self.v_feat, freeze=False)
        if self.t_feat is not None:
            self.text_embedding = nn.Embedding.from_pretrained(self.t_feat, freeze=False)

        if os.path.exists(mm_adj_file):
            self.mm_adj = torch.load(mm_adj_file)
        else:
            print("EXPERIMENT: Re-enabling fusion similarity for item graph construction...")
            if self.v_feat is not None:
                # Re-enable fusion similarity
                use_fusion_similarity = config['use_fusion_similarity'] if 'use_fusion_similarity' in config else True
                if hasattr(config, 'use_fusion_similarity') and use_fusion_similarity:
                    print("Building item graph using fusion similarity...")
                    from ..utils.fusion_similarity import compute_fusion_similarity
                    fusion_weight = config['fusion_weight'] if 'fusion_weight' in config else 0.5
                    similarity_matrix = compute_fusion_similarity(
                        self.v_feat, self.t_feat, 
                        fusion_weight=fusion_weight
                    )
                    print("Fusion similarity computed successfully")
                else:
                    print("Using simple cosine similarity (no fusion)...")
                    # Use simple cosine similarity as fallback
                    image_feats = self.image_embedding.weight.detach()
                    image_feats = F.normalize(image_feats, p=2, dim=1)
                    similarity_matrix = torch.mm(image_feats, image_feats.t())
                
                
                _, knn_indices = torch.topk(similarity_matrix, k=self.knn_k+1, dim=-1)
                knn_indices = knn_indices[:, 1:]  # Remove self-connections
                
                # Build simple adjacency matrix
                edge_list = []
                for i, neighbors in enumerate(knn_indices):
                    for neighbor in neighbors:
                        edge_list.append([i, neighbor.item()])
                edge_list = torch.tensor(edge_list).t()
                self.mm_adj = torch.sparse_coo_tensor(
                    edge_list, 
                    torch.ones(edge_list.size(1)), 
                    (self.n_items, self.n_items)
                ).coalesce()
                
            torch.save(self.mm_adj, mm_adj_file)
            print("Successfully built simple cosine similarity graph")
        self.mm_adj = self.mm_adj.to(self.device)
        # Load and enhance the dataset using SVD matrix completion
        print("EXPERIMENT: Re-enabling SVD completion...")
        train_interactions_sparse = dataset.inter_matrix(form='coo').astype(np.float32)
        
        # Re-enable SVD completion
        use_svd_completion = config['use_svd_completion'] if 'use_svd_completion' in config else True
        if use_svd_completion:
            print("Applying SVD completion to enhance sparse interactions...")
            svd_k = config['svd_k'] if 'svd_k' in config else 50
            train_interactions = apply_svd_completion_to_dataset(dataset, k=svd_k)
            print("SVD completion applied successfully")
        else:
            print("Using original interactions (no SVD completion)")
            train_interactions = train_interactions_sparse
            
        edge_index = self.pack_edge_index(train_interactions)
        self.edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous().to(self.device)
        self.edge_index = torch.cat((self.edge_index, self.edge_index[[1, 0]]), dim=1)

        # Simplified GCN initialization - use embedding_size directly
        if self.v_feat is not None:
            self.v_gcn = GCN(self.dataset, batch_size, num_user, num_item, dim_x, self.aggr_mode, 
                           dim_latent=dim_x, device=self.device, features=self.v_feat)
        if self.t_feat is not None:
            self.t_gcn = GCN(self.dataset, batch_size, num_user, num_item, dim_x, self.aggr_mode, 
                           dim_latent=dim_x, device=self.device, features=self.t_feat)

        # Simplified result embeddings - adjust dimensions for final concatenated representations
        # final_users will have dim 128 (64+64), final_items will have dim 128 (32+32+32+32)
        final_dim = dim_x * 2  # 128 for concatenated representations
        self.result_embed = nn.Parameter(
            nn.init.xavier_normal_(torch.tensor(np.random.randn(num_user + num_item, final_dim), dtype=torch.float32))).to(self.device)
        
        # Initialize adaptive BPR loss (friend's improvement #3)
        print("Initializing adaptive BPR loss...")
        # Extract the underlying dataset from the dataloader if needed
        actual_dataset = dataset.dataset if hasattr(dataset, 'dataset') else dataset
        self.adaptive_bpr_loss = AdaptiveBPRLoss(actual_dataset, gamma=0.5)
        print("Successfully initialized adaptive BPR loss")
        
        # Apply the model's initialization
        self.apply(xavier_uniform_initialization)

    def get_knn_adj_mat(self, mm_embeddings):
        context_norm = mm_embeddings.div(torch.norm(mm_embeddings, p=2, dim=-1, keepdim=True))
        sim = torch.mm(context_norm, context_norm.transpose(1, 0))
        _, knn_ind = torch.topk(sim, self.knn_k, dim=-1)
        adj_size = sim.size()
        del sim
        # construct sparse adj
        indices0 = torch.arange(knn_ind.shape[0]).to(self.device)
        indices0 = torch.unsqueeze(indices0, 1)
        indices0 = indices0.expand(-1, self.knn_k)
        indices = torch.stack((torch.flatten(indices0), torch.flatten(knn_ind)), 0)
        # norm
        return indices, self.compute_normalized_laplacian(indices, adj_size)

    def compute_normalized_laplacian(self, indices, adj_size):
        adj = torch.sparse.FloatTensor(indices, torch.ones_like(indices[0]), adj_size)
        row_sum = 1e-7 + torch.sparse.sum(adj, -1).to_dense()
        r_inv_sqrt = torch.pow(row_sum, -0.5)
        rows_inv_sqrt = r_inv_sqrt[indices[0]]
        cols_inv_sqrt = r_inv_sqrt[indices[1]]
        values = rows_inv_sqrt * cols_inv_sqrt
        return torch.sparse.FloatTensor(indices, values, adj_size)

    def normalize_adj(self, adj):
        """
        Paper Equation 10: Symmetric normalization D^{-1/2} A D^{-1/2}
        """
        # Get degree matrix
        deg = torch.sparse.sum(adj, dim=1).to_dense()
        deg_inv_sqrt = torch.pow(deg + 1e-7, -0.5)  # Add small epsilon for numerical stability
        deg_inv_sqrt[torch.isinf(deg_inv_sqrt)] = 0
        
        # Create diagonal matrix
        indices = torch.arange(len(deg_inv_sqrt), device=adj.device).unsqueeze(0).repeat(2, 1)
        deg_mat = torch.sparse.FloatTensor(indices, deg_inv_sqrt, adj.size()).to(adj.device)
        
        # D^{-1/2} A D^{-1/2}
        return torch.sparse.mm(deg_mat, torch.sparse.mm(adj, deg_mat))

    def pre_epoch_processing(self):
        """Dummy method to satisfy trainer requirements"""
        pass
    
    def post_epoch_processing(self):
        """Dummy method to satisfy trainer requirements"""
        return None

    # Remove pre_epoch_processing - no user graph needed
    # def pre_epoch_processing(self):

    def pack_edge_index(self, inter_mat):
        rows = inter_mat.row
        cols = inter_mat.col + self.n_users
        # ndarray([598918, 2]) for ml-imdb
        return np.column_stack((rows, cols))

    def forward(self, interaction):
        user_nodes, pos_item_nodes, neg_item_nodes = interaction[0], interaction[1], interaction[2]
        pos_item_nodes += self.n_users
        neg_item_nodes += self.n_users

        # Paper Section 4.1.1: Heterogeneous User-Item Graph (Equations 6-7)
        # Use the same edge_index for both modalities (no dropout as in paper)
        v_rep, _ = self.v_gcn(self.edge_index, self.edge_index, self.v_feat)
        t_rep, _ = self.t_gcn(self.edge_index, self.edge_index, self.t_feat)

        # Paper Section 4.2.1: Split modal representations using learned transformations
        # Apply learned splitting to item representations only (for distancing loss)
        item_v_rep = v_rep[self.num_user:]  # Item visual representations
        item_t_rep = t_rep[self.num_user:]  # Item text representations
        
        v_generic = self.v_generic_split(item_v_rep)  # Learned visual generic
        v_unique = self.v_unique_split(item_v_rep)    # Learned visual unique
        t_generic = self.t_generic_split(item_t_rep)  # Learned text generic
        t_unique = self.t_unique_split(item_t_rep)    # Learned text unique

        # Paper Section 4.1.2: Homogeneous Item-Item Graph (Equations 8-12)
        # Apply to concatenated item representations
        item_concat = torch.cat((v_rep[self.num_user:], t_rep[self.num_user:]), dim=1)
        item_enhanced = self.buildItemGraph(item_concat)

        # Paper Equation 17: Final fusion
        # E_i = {E_q^t || E_g^t || E_q^v || E_g^v + H}
        # E_u = {W_t · E_u^t || W_v · E_u^v}
        
        # Item representation: unique_text || generic_text || unique_visual || generic_visual + enhanced
        final_items = torch.cat((t_unique, t_generic, v_unique, v_generic), dim=1) + item_enhanced
        
        # User representation: weighted combination of modalities
        user_v = v_rep[:self.num_user]  # User visual rep
        user_t = t_rep[:self.num_user]  # User text rep
        # Simple equal weighting for users (can be made learnable)
        final_users = torch.cat((0.5 * user_t, 0.5 * user_v), dim=1)

        # Update result embeddings
        self.result_embed.data = torch.cat((final_users, final_items), dim=0)

        # Calculate BPR scores
        user_tensor = self.result_embed[user_nodes]
        pos_item_tensor = self.result_embed[pos_item_nodes]
        neg_item_tensor = self.result_embed[neg_item_nodes]
        pos_scores = torch.sum(user_tensor * pos_item_tensor, dim=1)
        neg_scores = torch.sum(user_tensor * neg_item_tensor, dim=1)

        return pos_scores, neg_scores, v_generic, v_unique, t_generic, t_unique

    def buildItemGraph(self, h):
        """
        Paper Section 4.1.2: Homogeneous Item-Item Graph with proper normalization
        Equation 12: H^(k) = S · H^(k-1)
        """
        # Apply symmetric normalization (Equation 10)
        norm_adj = self.normalize_adj(self.mm_adj)
        
        # Apply multiple layers without residual (revert to original)
        for i in range(self.n_layers):
            h = torch.sparse.mm(norm_adj, h)  # Equation 12
            
        return h  # Remove residual connection

    def calculate_loss(self, interaction):
        pos_scores, neg_scores, v_generic, v_unique, t_generic, t_unique = self.forward(interaction)
        
        # Get user indices for adaptive BPR loss
        users = interaction[0]  # User indices from interaction tuple
        
        # Paper Equation 18: Adaptive BPR loss (friend's improvement #3)
        bpr_loss = self.adaptive_bpr_loss(pos_scores, neg_scores, users)

        # Paper Equation 16: Alignment loss (SoloSimLoss) - maximize MI between generic parts
        align_loss = self.Solosimloss(v_generic, t_generic)

        # Paper Equation 15: Distancing loss - minimize MI between generic and unique within same modality
        # Using CLUB (Contrastive Log-ratio Upper Bound) for mutual information minimization
        v_distance_loss = self.club_v.forward(v_generic, v_unique)
        t_distance_loss = self.club_t.forward(t_generic, t_unique)
        
        # Paper Equation 19: Combined loss function
        # L = L_bpr + α * L_align + β * (L_v_dis + L_t_dis)
        loss = bpr_loss + self.alpha_contrast * align_loss + self.beta * (v_distance_loss + t_distance_loss)
        
        return loss

    def full_sort_predict(self, interaction):
        user_tensor = self.result_embed[:self.n_users]
        item_tensor = self.result_embed[self.n_users:]

        temp_user_tensor = user_tensor[interaction[0], :]
        score_matrix = torch.matmul(temp_user_tensor, item_tensor.t())
        return score_matrix

    # Removed topk_sample and _generate_simple_user_graph methods - not needed for simplified SEA model
    
    def Solosimloss(self, E_g_v, E_g_t):
        """
        Paper Equation 16: SoloSimLoss for alignment
        Maximizes MI between generic parts using single positive pair per anchor
        """
        # Paper's approach: Only align corresponding items, no negative sampling
        # Normalize features
        E_g_v = F.normalize(E_g_v, dim=1)
        E_g_t = F.normalize(E_g_t, dim=1)
        
        # Compute similarity for corresponding pairs only
        logits = torch.sum(E_g_v * E_g_t, dim=1) / self.temp
        
        # Add stability: clamp logits to prevent extreme values
        logits = torch.clamp(logits, min=-10.0, max=10.0)
        
        # Paper formulation: maximize similarity, so minimize negative similarity
        return -logits.mean()  # Negative for maximization (minimize negative = maximize positive)

    # Removed aggregate_user_with_graph method - not needed for simplified SEA model

class GCN(torch.nn.Module):
    def __init__(self, datasets, batch_size, num_user, num_item, dim_id, aggr_mode,
                 dim_latent=None, device=None, features=None):
        super(GCN, self).__init__()
        self.batch_size = batch_size
        self.num_user = num_user
        self.num_item = num_item
        self.datasets = datasets
        self.dim_id = dim_id
        self.dim_feat = features.size(1)
        self.dim_latent = dim_latent
        self.aggr_mode = aggr_mode
        self.device = device

        if self.dim_latent:
            self.preference = nn.Parameter(nn.init.xavier_normal_(torch.tensor(
                np.random.randn(num_user, self.dim_latent), dtype=torch.float32, requires_grad=True),
                gain=1).to(self.device))
            self.MLP = nn.Linear(self.dim_feat, 4 * self.dim_latent)
            self.MLP_1 = nn.Linear(4 * self.dim_latent, self.dim_latent)
            self.conv_embed_1 = StandardGCN(self.dim_latent, self.dim_latent, aggr=self.aggr_mode)
            self.conv_embed_2 = StandardGCN(self.dim_latent, self.dim_latent, aggr=self.aggr_mode)

        else:
            self.preference = nn.Parameter(nn.init.xavier_normal_(torch.tensor(
                np.random.randn(num_user, self.dim_feat), dtype=torch.float32, requires_grad=True),
                gain=1).to(self.device))
            self.conv_embed_1 = StandardGCN(self.dim_feat, self.dim_feat, aggr=self.aggr_mode)
            self.conv_embed_2 = StandardGCN(self.dim_feat, self.dim_feat, aggr=self.aggr_mode)

    def forward(self, edge_index_drop, edge_index, features, perturbed=False):
        temp_features = self.MLP_1(F.leaky_relu(self.MLP(features))) if self.dim_latent else features
        x = torch.cat((self.preference, temp_features), dim=0).to(self.device)
        x = F.normalize(x).to(self.device)

        # Paper Equations 6-7: Two-layer GCN with residual connections
        h1 = self.conv_embed_1(x, edge_index)
        h2 = self.conv_embed_2(h1, edge_index)
        
        # Sum aggregation as per Equation 7
        x_hat = x + h1 + h2
        return x_hat, self.preference


class StandardGCN(nn.Module):
    """
    Simplified GCN implementation following Paper Equations 6-7
    Avoids PyTorch Geometric compatibility issues
    """
    def __init__(self, in_channels, out_channels, normalize=True, bias=True, aggr='add', **kwargs):
        super(StandardGCN, self).__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.normalize = normalize

    def forward(self, x, edge_index, size=None):
        """
        Simple GCN forward pass with edge-based aggregation
        """
        row, col = edge_index
        
        if self.normalize:
            # Compute degrees for normalization
            deg_row = torch.bincount(row, minlength=x.size(0)).float()
            deg_col = torch.bincount(col, minlength=x.size(0)).float()
            
            # Normalization: 1/sqrt(|N_u| * |N_i|) as per Equations 6-7
            norm = 1.0 / (torch.sqrt(deg_row[row]) * torch.sqrt(deg_col[col]) + 1e-7)
        else:
            norm = torch.ones(edge_index.size(1), device=x.device)
        
        # Aggregate messages: for each edge, take source node features and normalize
        messages = x[row] * norm.view(-1, 1)
        
        # Aggregate by target nodes
        out = torch.zeros_like(x)
        out.index_add_(0, col, messages)
        
        return out

    def __repr__(self):
        return '{}({},{})'.format(self.__class__.__name__, self.in_channels, self.out_channels)
